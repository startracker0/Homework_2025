import os
import torch
import torch.nn as nn
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
import argparse
import json
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.colors import LinearSegmentedColormap

def imshow(inp, ax, title=None):
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    ax.imshow(inp)
    if title is not None:
        ax.set_title(title, fontsize=20)  # å°†å­—ä½“å¤§å°ä»10å¢åŠ åˆ°14
    ax.axis('off')
    
def save_prediction_visuals(dataset, all_preds, all_labels, idx_to_class, num_images=5, save_path='./outputs/predictions.png'):
    plt.figure(figsize=(3 * num_images, 3))
    for i in range(num_images):
        ax = plt.subplot(1, num_images, i + 1)
        img_tensor, _ = dataset[i]
        true_label = idx_to_class[all_labels[i]]
        pred_label = idx_to_class[all_preds[i]]
        title = f'{true_label} â†’ {pred_label}'
        imshow(img_tensor, ax, title=title)
    plt.tight_layout()
    plt.savefig(save_path)
    print(f'âœ… å¯è§†åŒ–é¢„æµ‹ç»“æœå·²ä¿å­˜ä¸º: {save_path}')


# å‚æ•°è®¾ç½®
parser = argparse.ArgumentParser(description='æµ‹è¯•èŠ±å‰åˆ†ç±»æ¨¡å‹')
parser.add_argument('--data_dir', type=str, default='./flower_data', help='æ•°æ®é›†è·¯å¾„')
parser.add_argument('--model_path', type=str,default='./outputs/best_model_resnet50.pth', help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„')
parser.add_argument('--model_type', type=str, default='resnet50', choices=['resnet18', 'resnet34', 'resnet50', 'resnet101'], help='ResNetæ¨¡å‹ç±»å‹')
parser.add_argument('--batch_size', type=int, default=32, help='æµ‹è¯•æ‰¹é‡å¤§å°')
parser.add_argument('--class_mapping', type=str, default='./outputs/class_mapping.json', help='ç±»åˆ«æ˜ å°„æ–‡ä»¶')
args = parser.parse_args()

# è®¾ç½®è®¾å¤‡
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# æ•°æ®é¢„å¤„ç†
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# åŠ è½½ç±»åˆ«æ˜ å°„
with open(args.class_mapping, 'r') as f:
    class_to_idx = json.load(f)
    
num_classes = len(class_to_idx)
idx_to_class = {v: k for k, v in class_to_idx.items()}
print(f"è®­ç»ƒæ—¶çš„ç±»åˆ«æ•°é‡: {num_classes}")

# åŠ è½½æµ‹è¯•é›†
test_dataset = datasets.ImageFolder(os.path.join(args.data_dir, 'test'), transform)
test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)
print(f"æµ‹è¯•é›†ä¸­çš„ç±»åˆ«æ•°é‡: {len(test_dataset.classes)}")

# åŠ è½½æ¨¡å‹ - ä½¿ç”¨è®­ç»ƒæ—¶çš„ç±»åˆ«æ•°é‡
print(f"åŠ è½½ {args.model_type} æ¨¡å‹...")
if args.model_type == 'resnet18':
    model = models.resnet18()
elif args.model_type == 'resnet34':
    model = models.resnet34()
elif args.model_type == 'resnet101':
    model = models.resnet101()
else:
    model = models.resnet50()

# ä½¿ç”¨è®­ç»ƒæ—¶çš„ç±»åˆ«æ•°é‡åˆå§‹åŒ–æœ€åçš„å…¨è¿æ¥å±‚
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, num_classes)  # ä½¿ç”¨è®­ç»ƒæ—¶çš„ç±»åˆ«æ•°é‡
model.load_state_dict(torch.load(args.model_path, map_location=device))
model = model.to(device)
model.eval()

# åˆ›å»ºæµ‹è¯•é›†ç±»åˆ«åˆ°è®­ç»ƒé›†ç±»åˆ«çš„æ˜ å°„
test_class_to_idx = test_dataset.class_to_idx
test_class_mapping = {}
for test_class, test_idx in test_class_to_idx.items():
    if test_class in class_to_idx:
        test_class_mapping[test_idx] = class_to_idx[test_class]
    else:
        print(f"è­¦å‘Š: æµ‹è¯•é›†ä¸­çš„ç±»åˆ« '{test_class}' åœ¨è®­ç»ƒé›†ä¸­ä¸å­˜åœ¨")

# æµ‹è¯•
correct = 0
total = 0
all_preds = []
all_labels = []

with torch.no_grad():
    for inputs, labels in tqdm(test_loader, desc='Testing'):
        inputs = inputs.to(device)
        
        # è·³è¿‡åœ¨è®­ç»ƒé›†ä¸­ä¸å­˜åœ¨çš„ç±»åˆ«
        valid_indices = torch.tensor([i for i, label in enumerate(labels) 
                                     if label.item() in test_class_mapping.keys()])
        
        if len(valid_indices) == 0:
            continue
            
        selected_inputs = inputs[valid_indices]
        selected_labels = torch.tensor([test_class_mapping[label.item()] 
                                       for label in labels[valid_indices]], 
                                       device=device)
        
        outputs = model(selected_inputs)
        _, preds = torch.max(outputs, 1)

        total += selected_labels.size(0)
        correct += (preds == selected_labels).sum().item()
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(selected_labels.cpu().numpy())

# è¾“å‡ºæµ‹è¯•å‡†ç¡®ç‡
acc = correct / total if total > 0 else 0
print(f'\nâœ… æµ‹è¯•é›†æ€»æ ·æœ¬: {total}')
print(f'âœ… å‡†ç¡®é¢„æµ‹: {correct}')
print(f'âœ… æµ‹è¯•é›†å‡†ç¡®ç‡: {acc:.4f}')

# æ‰“å°éƒ¨åˆ†é¢„æµ‹ç»“æœ
print("\nğŸ“‹ éƒ¨åˆ†é¢„æµ‹ç¤ºä¾‹ï¼ˆå®é™… â†’ é¢„æµ‹ï¼‰:")
for i in range(min(10, len(all_labels))):
    true_label = idx_to_class[all_labels[i]]
    pred_label = idx_to_class[all_preds[i]]
    print(f'{true_label} â†’ {pred_label}')
    
save_prediction_visuals(test_dataset, all_preds, all_labels, idx_to_class)

